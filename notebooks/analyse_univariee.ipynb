{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41b1648a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Configurer explicitement les chemins afin d'éviter les mauvaises surprises liées à l'environnement lors des réexécutions du notebook.\n",
    "DATA_DIRECTORY: Path = Path(\"..\") / \"data\"\n",
    "RAW_DATASET_PATH: Path = DATA_DIRECTORY / \"dataset.csv\"\n",
    "CLEANED_DATASET_PATH: Path = DATA_DIRECTORY / \"dataset_cleaned.csv\"\n",
    "PLOTS_DISTRIBUTION_DIR: Path = Path(\"..\") / \"plots\" / \"distribution\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c071dd15",
   "metadata": {},
   "source": [
    "# Exploration univariée – Jeu de données Weigh Lifestyle\n### Objectifs\n- Quantifier la distribution des variables comportementales, physiologiques et nutritionnelles.\n- Détecter les valeurs improbables avant la modélisation multivariée.\n- Produire des artefacts réutilisables : jeu de données nettoyé et graphiques de distribution.\n\n### Ressources de données\n- Jeu de données brut : `../data/dataset.csv`\n- Export nettoyé : `../data/dataset_cleaned.csv`\n- Graphiques générés : `../plots/distribution/`\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb33af9",
   "metadata": {},
   "source": [
    "## Plan d'analyse\n### Étape 1 – Charger les observations brutes\n- Lire le CSV dans un DataFrame et exposer la structure pour les étapes suivantes.\n\n### Étape 2 – Éliminer les fuites et les variables peu informatives\n- Supprimer les colonnes qui divulguent la cible ou ajoutent du bruit redondant.\n\n### Étape 3 – Harmoniser les échelles des variables\n- Arrondir les métriques discrètes et limiter les mesures continues à deux décimales.\n- Enregistrer le jeu de données nettoyé pour le réutiliser dans les autres notebooks.\n\n### Étape 4 – Inspecter les distributions\n- Générer des histogrammes pour chaque variable numérique informative.\n- Sauvegarder les graphiques dans le dossier de distribution.\n\n### Étape 5 – Mettre en évidence les valeurs aberrantes extrêmes\n- Appliquer une règle IQR extrême pour signaler les valeurs à examiner manuellement.\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef00eb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path: Path = RAW_DATASET_PATH) -> pd.DataFrame:\n",
    "    \"\"\"Load dataset from CSV file.\"\"\"\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "df = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "427dc33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 32 columns\n",
      "Remaining columns: 23\n"
     ]
    }
   ],
   "source": [
    "def clean_dataset(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Remove columns that would leak target information or add noise.\"\"\"\n",
    "    # Empêcher les variables morphologiques de divulguer directement la taille corporelle.\n",
    "    # Retirer les proxys de résultats d'entraînement pour concentrer la modélisation sur les signaux comportementaux.\n",
    "    cols_to_drop = [\n",
    "        \"expected_burn\",\n",
    "        \"meal_name\",  # Single-category column provides no discriminative power.\n",
    "        \"meal_type\",  # Encodes meal ordering rather than nutritional detail.\n",
    "        \"Name of Exercise\",  # Verbose label with minimal predictive value.\n",
    "        \"Sets\",  # Workout log metadata noisy for univariate analysis.\n",
    "        \"Reps\",  # Workout log metadata noisy for univariate analysis.\n",
    "        \"Benefit\",  # Subjective textual scale unsuitable for numeric distribution analysis.\n",
    "        \"Target Muscle Group\",  # High-cardinality categorical data without aggregation value here.\n",
    "        \"Equipment Needed\",  # Captures equipment catalog rather than participant behaviour.\n",
    "        \"Difficulty Level\",  # Instructor-provided rating not comparable across entries.\n",
    "        \"Body Part\",  # Redundant with target muscle field in this context.\n",
    "        \"Type of Muscle\",  # Biological category overlapping with target muscle group.\n",
    "        \"Workout\",  # Broad workout taxonomy already encoded elsewhere.\n",
    "        \"pct_carbs\",  # Redundant with carbohydrate gram feature.\n",
    "        \"cal_from_macros\",  # Decision made based on the correlation matrix.\n",
    "        \"pct_HRR\",  # Decision made based on the correlation matrix.\n",
    "        \"BMI\", # Too much correlated with BMI_calc\n",
    "        \"Burns Calories (per 30 min)_bc\", # Weird values\n",
    "        \"Height (m)\", # Too much information with weight\n",
    "        \"Fat_Percentage\", # Too much information with weight\n",
    "        \"Calories\", # Too much information\n",
    "        \"cal_balance\", # Too much information\n",
    "        \"BMI_calc\", # Too much information\n",
    "        \"lean_mass_kg\", # Too much information\n",
    "        \"protein_per_kg\",\n",
    "        \"Calories_Burned\", # Give indirect information about weight\n",
    "        \"Burns Calories (per 30 min)_bc\", # Give indirect information about weight\n",
    "        \"Burns_Calories_Bin\", # Give indirect information about weight\n",
    "        \"Burns Calories (per 30 min)\", # Give indirect information about weight\n",
    "        \"Carbs\", # Too much information with weight\n",
    "        \"Proteins\", # Too much information with weight\n",
    "        \"Fats\", # Too much information with weight\n",
    "\n",
    "]\n",
    "    \n",
    "    # Programmer de manière défensive pour éviter les erreurs si les colonnes ont déjà été supprimées\n",
    "    cols_to_drop_existing = [col for col in cols_to_drop if col in df.columns]\n",
    "    df_cleaned = df.drop(columns=cols_to_drop_existing)\n",
    "    \n",
    "    print(f\"Dropped {len(cols_to_drop_existing)} columns\")\n",
    "    print(f\"Remaining columns: {df_cleaned.shape[1]}\")\n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "df_cleaned = clean_dataset(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e72120d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_dataset_values(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Return a rounded copy that respects discrete and continuous measurement scales.\"\"\"\n",
    "    df_rounded = df.copy()\n",
    "\n",
    "    # Utiliser la partie entière pour l'âge afin de ne pas surestimer les segments démographiques.\n",
    "    if 'Age' in df_rounded.columns:\n",
    "        df_rounded['Age'] = np.floor(df_rounded['Age']).astype(int)\n",
    "    \n",
    "    # Forcer les entiers sur les mesures discrètes relevées manuellement.\n",
    "    cols_to_int = [\n",
    "        'Max_BPM', 'Avg_BPM', 'Resting_BPM',\n",
    "        'Workout_Frequency (days/week)', 'Experience_Level',\n",
    "        'Daily meals frequency', 'Physical exercise',\n",
    "        'Calories', 'sodium_mg', 'cholesterol_mg',\n",
    "        'prep_time_min', 'cook_time_min'\n",
    "    ]\n",
    "    for col in cols_to_int:\n",
    "        if col in df_rounded.columns:\n",
    "            df_rounded[col] = df_rounded[col].round(0).astype(int)\n",
    "    \n",
    "    # Limiter les mesures continues à deux décimales pour réduire le bruit tout en restant lisible.\n",
    "    cols_to_2dec = [\n",
    "        'Session_Duration (hours)', 'Water_Intake (liters)',\n",
    "        'sugar_g', 'serving_size_g', 'rating',\n",
    "        'pct_maxHR', \"burns-calories-(per-30-min)-bc\"\n",
    "    ]\n",
    "    for col in cols_to_2dec:\n",
    "        if col in df_rounded.columns:\n",
    "            df_rounded[col] = df_rounded[col].round(2)\n",
    "    \n",
    "    \n",
    "    return df_rounded\n",
    "\n",
    "df_cleaned = round_dataset_values(df_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18d3c0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset rounded and saved to ../data/dataset_cleaned.csv\n",
      "   Age  Gender  Weight (kg)  Max_BPM  Avg_BPM  Resting_BPM  \\\n",
      "0   34    Male        65.27      189      158           69   \n",
      "1   23  Female        56.41      179      132           73   \n",
      "2   33  Female        58.98      175      124           55   \n",
      "3   38  Female        93.78      191      155           50   \n",
      "4   45    Male        52.42      194      153           71   \n",
      "\n",
      "   Session_Duration (hours) Workout_Type  Water_Intake (liters)  \\\n",
      "0                      1.00     Strength                   1.50   \n",
      "1                      1.37         HIIT                   1.90   \n",
      "2                      0.91       Cardio                   1.88   \n",
      "3                      1.10         HIIT                   2.50   \n",
      "4                      1.08     Strength                   2.91   \n",
      "\n",
      "   Workout_Frequency (days/week)  ...   diet_type  sugar_g  sodium_mg  \\\n",
      "0                              4  ...       Vegan    31.77       1730   \n",
      "1                              4  ...  Vegetarian    12.34        693   \n",
      "2                              3  ...       Paleo    42.81       2142   \n",
      "3                              4  ...       Paleo     9.34        123   \n",
      "4                              4  ...       Vegan    23.78       1935   \n",
      "\n",
      "  cholesterol_mg  serving_size_g  cooking_method  prep_time_min  \\\n",
      "0            285          120.47         Grilled             16   \n",
      "1            301          109.15           Fried             16   \n",
      "2            215          399.43          Boiled             54   \n",
      "3             10          314.31           Fried             28   \n",
      "4            117           99.22           Baked             34   \n",
      "\n",
      "   cook_time_min rating  pct_maxHR  \n",
      "0            111   1.31       0.84  \n",
      "1             12   1.92       0.73  \n",
      "2              6   4.70       0.71  \n",
      "3            104   4.85       0.81  \n",
      "4             47   3.07       0.79  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "# Sauvegarder le jeu de données arrondi pour réutiliser exactement les mêmes données nettoyées.\n",
    "df_cleaned.to_csv(CLEANED_DATASET_PATH, index=False)\n",
    "print(f\"Dataset rounded and saved to {CLEANED_DATASET_PATH}\")\n",
    "print(df_cleaned.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e748ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 19 distribution plots to ../plots/distribution\n"
     ]
    }
   ],
   "source": [
    "def _sanitize_filename(column_name: str) -> str:\n",
    "    \"\"\"Convert column name to safe filename by removing problematic characters.\"\"\"\n",
    "    return column_name.replace('/', '_').replace(' ', '_').replace('(', '').replace(')', '')\n",
    "\n",
    "\n",
    "def _get_columns_with_variance(df: pd.DataFrame) -> list[str]:\n",
    "    \"\"\"Return list of numerical columns with variance > 0.\"\"\"\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    # Les colonnes constantes (variance = 0) n'apportent aucune information de distribution.\n",
    "    return [col for col in numerical_cols if df[col].var() > 0]\n",
    "\n",
    "\n",
    "def plot_numerical_distributions(\n",
    "    df: pd.DataFrame, output_dir: Path = PLOTS_DISTRIBUTION_DIR\n",
    ") -> tuple[list[Path], dict[str, str]]:\n",
    "    \"\"\"Plot histograms for numerical columns with variance and record any failures.\n",
    "    \n",
    "    Avoids constant columns to keep the visual portfolio meaningful.\n",
    "    Returns both the saved figure paths and any plotting errors for transparency.\n",
    "    \"\"\"\n",
    "    import matplotlib\n",
    "    matplotlib.use('Agg')  # Non-interactive backend to avoid Jupyter conflicts\n",
    "    from matplotlib import pyplot as plt_local\n",
    "    \n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    cols_to_plot = _get_columns_with_variance(df)\n",
    "\n",
    "    saved_paths: list[Path] = []\n",
    "    errors: dict[str, str] = {}\n",
    "\n",
    "    for col in cols_to_plot:\n",
    "        fig = None\n",
    "        try:\n",
    "            # Convertir rapidement en tableau NumPy pour éviter les problèmes d'histogramme entre pandas et matplotlib.\n",
    "            data = np.array(df[col].dropna().values, dtype=float)\n",
    "            if len(data) == 0:\n",
    "                continue\n",
    "\n",
    "            fig, ax = plt_local.subplots(figsize=(10, 6))\n",
    "            ax.hist(data, bins=30, edgecolor='black', alpha=0.7)\n",
    "            ax.set_title(f'Distribution of {col}', fontsize=14, fontweight='bold')\n",
    "            ax.set_xlabel(col, fontsize=12)\n",
    "            ax.set_ylabel('Frequency', fontsize=12)\n",
    "            ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "            filepath = output_path / f'{_sanitize_filename(col)}_distribution.png'\n",
    "            fig.savefig(filepath, dpi=150, bbox_inches='tight')\n",
    "            saved_paths.append(filepath)\n",
    "        except Exception as exc:\n",
    "            errors[col] = str(exc)\n",
    "        finally:\n",
    "            if fig is not None:\n",
    "                plt_local.close(fig)\n",
    "\n",
    "    return saved_paths, errors\n",
    "\n",
    "\n",
    "# Remonter l'état du traçage à l'extérieur pour garder la fonction sans effets de bord.\n",
    "saved_plots, plot_errors = plot_numerical_distributions(df_cleaned)\n",
    "print(f\"Saved {len(saved_plots)} distribution plots to {PLOTS_DISTRIBUTION_DIR}\")\n",
    "if plot_errors:\n",
    "    print(\"Plotting issues detected:\")\n",
    "    for column, message in plot_errors.items():\n",
    "        print(f\"  {column}: {message}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f06c5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found outliers in 1 column(s):\n",
      "\n",
      "Physical exercise:\n",
      "  Count: 4695 (23.47%)\n",
      "  Range: [1.0, 4.0]\n",
      "  Valid bounds: [0.0, 0.0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def detect_outliers_iqr(df: pd.DataFrame) -> dict[str, dict[str, float]]:\n    \"\"\"Detect extreme outliers using IQR method for all numerical columns.\n\n    Returns a report only for columns containing outliers.\n    Extreme outliers are defined as values < Q1 - 3.0*IQR or > Q3 + 3.0*IQR.\n    Using factor 3.0 instead of 1.5 to detect only truly extreme values.\n\n    Args:\n        df: Input DataFrame with numerical columns\n\n    Returns:\n        Dictionary with column names as keys and outlier statistics as values.\n        Statistics include: count, percentage, min_outlier, max_outlier,\n        lower_bound, upper_bound.\n    \"\"\"\n    outlier_report = {}\n    numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n\n    for col in numerical_cols:\n        # Ignorer les colonnes sans variance car elles ne peuvent pas contenir de valeurs aberrantes.\n        if df[col].var() == 0:\n            continue\n\n        data = df[col].dropna()\n\n        # Calculer les bornes de l'IQR avec un facteur 3.0 pour ne cibler que les valeurs extrêmes.\n        Q1 = data.quantile(0.25)\n        Q3 = data.quantile(0.75)\n        IQR = Q3 - Q1\n        lower_bound = Q1 - 3.0 * IQR\n        upper_bound = Q3 + 3.0 * IQR\n\n        # Identifier les valeurs aberrantes\n        outliers = data[(data < lower_bound) | (data > upper_bound)]\n\n        # Ne produire un rapport que lorsqu'au moins une valeur aberrante est détectée.\n        if len(outliers) > 0:\n            outlier_report[col] = {\n                'count': len(outliers),\n                'percentage': round(len(outliers) / len(data) * 100, 2),\n                'min_outlier': float(outliers.min()),\n                'max_outlier': float(outliers.max()),\n                'lower_bound': round(lower_bound, 2),\n                'upper_bound': round(upper_bound, 2)\n            }\n\n    return outlier_report\n\n\n# Détecter et signaler les valeurs aberrantes\noutlier_report = detect_outliers_iqr(df_cleaned)\n\nif outlier_report:\n    print(f\"Found outliers in {len(outlier_report)} column(s):\\n\")\n    sorted_outliers = sorted(\n        outlier_report.items(),\n        key=lambda item: item[1]['count'],\n        reverse=True\n    )\n    max_rows = 15\n    for col, stats in sorted_outliers[:max_rows]:\n        print(f\"{col}:\")\n        print(f\"  Count: {stats['count']} ({stats['percentage']}%)\")\n        print(f\"  Range: [{stats['min_outlier']}, {stats['max_outlier']}]\")\n        print(f\"  Valid bounds: [{stats['lower_bound']}, {stats['upper_bound']}]\")\n        print()\n    if len(sorted_outliers) > max_rows:\n        print(f\"... {len(sorted_outliers) - max_rows} colonnes supplémentaires non affichées\")\nelse:\n    print(\"No outliers detected in any column.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684fafec",
   "metadata": {},
   "source": [
    "Il est possible de regrouper le nombre d'exercices physiques en 0 et > 0 afin de réduire le déséquilibre entre l'absence d'activité et les autres modalités."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Weigh_LifeStyle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}